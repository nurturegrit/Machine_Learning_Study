{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61b1bf6c-df18-4679-adaf-2c5644cfdd03",
   "metadata": {},
   "source": [
    "Naive Bayes Classifier\n",
    "Subtopics\n",
    "\n",
    "    Introduction to Naive Bayes\n",
    "    Bayes' Theorem\n",
    "    Types of Naive Bayes\n",
    "        Gaussian Naive Bayes\n",
    "        Multinomial Naive Bayes\n",
    "        Bernoulli Naive Bayes\n",
    "    Assumptions of Naive Bayes\n",
    "    Advantages and Disadvantages\n",
    "    Applications of Naive Bayes\n",
    "    Examples and Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f158dc98-65ab-4cbe-bba3-85a0a469f3f9",
   "metadata": {},
   "source": [
    "### 1. Introduction to Naive Bayes\n",
    "\n",
    "Naive Bayes classifiers are a family of probabilistic algorithms based on Bayes' Theorem, widely used for classification tasks. The power of this classifier lies in its simplicity and effectiveness, particularly for text classification problems (like spam detection, sentiment analysis, etc.). Despite its simplicity, Naive Bayes can outperform more sophisticated models in many scenarios, especially when dealing with large datasets and high dimensionality.\n",
    "Core Concept\n",
    "\n",
    "The core idea of Naive Bayes is to make predictions based on the conditional probabilities of the features given the classes. The model considers each feature independently when predicting the class label, which is a significant deviation from the more common approaches where the correlation between features is taken into account. This assumption of feature independence is what gives the model its \"naive\" label.\n",
    "\n",
    "To put it simply:\n",
    "    Given a class $ C $ and a feature vector $ X = (x_1, x_2, \\ldots, x_n) $, we want to compute the probability $ P(C | X) $.\n",
    "\n",
    "Using Bayes' Theorem, this can be expressed as:\n",
    "\n",
    "$ P(C | X) = \\frac{P(X | C) \\cdot P(C)}{P(X)} $\n",
    "\n",
    "Here:\n",
    "    $ P(C) $ is the prior probability of the class $ C $.\n",
    "    $ P(X | C) $ is the likelihood of the feature vector given class $ C $.\n",
    "    $ P(X) $ is the evidence, or the total probability of the feature vector across all classes.\n",
    "\n",
    "Conditional Independence Assumption\n",
    "\n",
    "The naive assumption is that the features $ x_i $ are conditionally independent given the class label $ C $, which simplifies the calculation of $ P(X | C) $:\n",
    "\n",
    "$ P(X | C) = P(x_1 | C) \\cdot P(x_2 | C) \\cdot \\ldots \\cdot P(x_n | C) $\n",
    "\n",
    "This means that the joint probability of the features can be reduced to the product of their individual probabilities.\n",
    "Example Scenario\n",
    "\n",
    "In a text classification example, say we want to classify whether an email is spam or not. Each word in the email can be considered a feature. Based on a training set of classified emails, we estimate the probabilities of each word appearing in spam and non-spam emails (the classes).\n",
    "\n",
    "    We start with total counts of spam and non-spam emails.\n",
    "    For each word in the vocabulary, we compute how often it appears in spam emails versus non-spam emails.\n",
    "    When classifying a new email, we multiply the probabilities of each word occurring in the two classes, weighing them by the prior probabilities of spam and non-spam emails.\n",
    "\n",
    "Mathematical Formulation\n",
    "\n",
    "To classify an instance $ X $, we choose the class $ C $ that maximizes the posterior probability $ P(C | X) $:\n",
    "\n",
    "$ C_{final} = \\arg\\max_C P(C | X) $\n",
    "\n",
    "By applying Bayes' Theorem and considering the conditional independence assumption:\n",
    "\n",
    "$ C_{final} = \\arg\\max_C \\left( P(C) \\prod_{i=1}^{n} P(x_i | C) \\right) $\n",
    "Summary\n",
    "\n",
    "Naive Bayes is an intuitive approach, excellent for large datasets with many features. Its independence assumption might seem too strong, but in practice, it often yields surprisingly good results. In the following sections, we'll explore Bayes' Theorem in more detail, the types of Naive Bayes classifiers, their assumptions, and their applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa00044c-95a2-4cb1-bc01-a1fdde4a89b7",
   "metadata": {},
   "source": [
    "2. Bayes' Theorem\n",
    "\n",
    "Bayes' Theorem is a fundamental concept in probability theory and statistics that describes how to update the probability of a hypothesis as more evidence or information becomes available. In the context of the Naive Bayes Classifier, it allows us to calculate the posterior probability of a class given the features of an instance.\n",
    "Bayes' Theorem Formula\n",
    "\n",
    "The theorem is represented mathematically as:\n",
    "\n",
    "$ P(C | X) = \\frac{P(X | C) \\cdot P(C)}{P(X)} $\n",
    "\n",
    "Where\n",
    " * $ P(C | X) $ is the posterior probability of class $ C $ given the features $ X $.\n",
    " * $ P(X | C) $ is the likelihood, the probability of observing the features given class ( C ).\n",
    " * $ P(C) $ is the prior probability of class ( C ), considering all prior knowledge before seeing the data.\n",
    " * $ P(X) $ is the marginal likelihood, the total probability of observing the feature set ( X ) under all classes.\n",
    "\n",
    "Understanding Each Component\n",
    "    Posterior Probability $ P(C | X) $:\n",
    "        This is what we aim to compute; it tells us how likely a particular class $ C $ is given the features $ X $.    \n",
    "    Likelihood $ P(X | C) $:\n",
    "        It reflects how likely the features $ X $ are under the assumption that the instance belongs to class $ C $.\n",
    "        For instance, in spam email classification, if the email contains specific words often associated with spam, the likelihood of these words appearing in the context of spam must be high.\n",
    "    Prior Probability $ P(C) $:\n",
    "        Prior probabilities are determined from the training data and represent our expectations about the probabilities of classes. For example, if 70% of emails in the training set are spam, then $ P(\\text{spam}) = 0.7 $ and $ P(\\text{not spam}) = 0.3 $.\n",
    "    Marginal Likelihood $ P(X) $:\n",
    "        This term can be seen as a normalizing constant ensuring that the probabilities across all classes sum to 1. It can be computed as:\n",
    "\n",
    "$ P(X) = \\sum_{C} P(X | C) \\cdot P(C) $\n",
    "\n",
    "This equation sums the product of the likelihood and prior probabilities over all classes.\n",
    "Example Calculation\n",
    "\n",
    "Suppose we have a binary classification problem with classes \"Spam\" and \"Not Spam.\" From the training data, we observe:\n",
    "    $ P(\\text{Spam}) = 0.6 $\n",
    "    $ P(\\text{Not Spam}) = 0.4 $\n",
    "    The likelihood of certain words given the classes might include $ P(X | \\text{Spam}) = 0.7 $ and $ P(X | \\text{Not Spam}) = 0.1 $.\n",
    "\n",
    "To classify a new email with features $ X $:\n",
    "    First, calculate the likelihood for both classes:\n",
    "        $ P(X | \\text{Spam}) = 0.7 $ (assuming we have calculated this from the training set).\n",
    "        $ P(X | \\text{Not Spam}) = 0.1 $.\n",
    "    Now you compute the marginal likelihood $ P(X) $:\n",
    "\n",
    "$ P(X) = P(X | \\text{Spam}) \\cdot P(\\text{Spam}) + P(X | \\text{Not Spam}) \\cdot P(\\text{Not Spam}) $\n",
    "\n",
    "Substituting in the values:\n",
    "\n",
    "$ P(X) = 0.7 \\cdot 0.6 + 0.1 \\cdot 0.4 = 0.42 + 0.04 = 0.46 $\n",
    "\n",
    "    Now, apply Bayes' Theorem to calculate the posterior probabilities:\n",
    "\n",
    "$ P(\\text{Spam} | X) = \\frac{P(X | \\text{Spam}) \\cdot P(\\text{Spam})}{P(X)} = \\frac{0.7 \\cdot 0.6}{0.46} \\approx 0.913 $\n",
    "\n",
    "$ P(\\text{Not Spam} | X) = \\frac{P(X | \\text{Not Spam}) \\cdot P(\\text{Not Spam})}{P(X)} = \\frac{0.1 \\cdot 0.4}{0.46} \\approx 0.087 $\n",
    "\n",
    "Given these calculations, since $ P(\\text{Spam} | X) \\approx 0.913 $ is greater than $ P(\\text{Not Spam} | X) \\approx 0.087 $, the email would be classified as \"Spam.\"\n",
    "Summary\n",
    "\n",
    "Bayes' Theorem provides a principled way to quantify our beliefs or knowledge in the presence of uncertainty and is central to the working of Naive Bayes classifiers. It emphasizes how we can update our belief about a hypothesis as more evidence becomes available."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1175cc7-093b-4f47-a99a-094415cdb755",
   "metadata": {},
   "source": [
    "3. Types of Naive Bayes\n",
    "\n",
    "Naive Bayes classifiers can be categorized into several types depending on the nature of the input features. The three main types are:\n",
    "\n",
    "    Gaussian Naive Bayes\n",
    "    Multinomial Naive Bayes\n",
    "    Bernoulli Naive Bayes\n",
    "\n",
    "Let’s explore each of these types in detail.\n",
    "\n",
    "3.1. Gaussian Naive Bayes\n",
    "\n",
    "Overview: Gaussian Naive Bayes is used when the features are continuous and are assumed to follow a Gaussian (normal) distribution. This is often applicable in cases where you have continuous data, such as height, weight, or temperature.\n",
    "\n",
    "Mathematical Foundation: For a continuous feature $ x $, the probability density function of a Gaussian distribution is given by:\n",
    "\n",
    "$ P(x | C) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} e^{-\\frac{(x - \\mu)^2}{2 \\sigma^2}} $\n",
    "\n",
    "Where:\n",
    "* $ \\mu $ is the mean of the feature in class $ C $,\n",
    "* $ \\sigma^2 $ is the variance of the feature in class $ C $.\n",
    "\n",
    "To predict the class for a new instance, Naive Bayes combines the prior class probability with the Gaussian probabilities of each feature:\n",
    "\n",
    "$ P(C | X) \\propto P(C) \\cdot P(x_1 | C) \\cdot P(x_2 | C) \\cdots P(x_n | C) $\n",
    "\n",
    "Example: Suppose we are trying to classify whether a person is healthy or unhealthy based on their cholesterol levels. If we assume cholesterol levels follow a Gaussian distribution for each class (healthy/unhealthy):\n",
    "* Calculate the mean and variance of cholesterol levels for both classes from the training data.\n",
    "* For a new data point, compute $ P(x | \\text{Healthy}) $ and $ P(x | \\text{Unhealthy}) $ using the Gaussian formula.\n",
    "* Finally, apply Bayes' Theorem to classify the data point.\n",
    "\n",
    "3.2. Multinomial Naive Bayes\n",
    "\n",
    "Overview: Multinomial Naive Bayes is specifically suited for document classification and problems where the features are discrete, often representing counts (like word counts, frequency of occurrences). This is common in text classification tasks.\n",
    "\n",
    "Mathematical Foundation: The probability of a feature vector $ X $ given class $ C $ can be computed using the multinomial distribution:\n",
    "\n",
    "$ P(X | C) = \\frac{(n!) \\prod_{i=1}^{k} (f_i!) }{(n + N)!} \\prod_{i=1}^{k} p_i^{f_i} $\n",
    "\n",
    "Where:\n",
    "* $ n $ is the total number of words (features).\n",
    "* $ N $ is the total number of unique words in the vocabulary.\n",
    "* $ f_i $ is the frequency of the $ i^{th} $ word (feature).\n",
    "* $ p_i $ is the probability of the $ i^{th} $ word given class $ C $, typically calculated from the frequency of occurrence in training data.\n",
    "\n",
    "Example: Consider a spam detection system analyzing emails:\n",
    "\n",
    "* Calculate the frequency of each word in spam and non-spam emails.\n",
    "* Use the frequencies to compute $ P(X | \\text{Spam}) $ and $ P(X | \\text{Not Spam}) $.\n",
    "* The classification is done similarly by applying Bayes' Theorem.\n",
    "\n",
    "3.3. Bernoulli Naive Bayes\n",
    "\n",
    "Overview: This variant is similar to Multinomial Naive Bayes but is tailored for binary/boolean feature vectors. It is suitable for cases where you want to capture the presence or absence of features rather than their counts.\n",
    "\n",
    "Mathematical Foundation: The probability can be represented as:\n",
    "\n",
    "$ P(X | C) = \\prod_{i=1}^{n} P(x_i | C)^{f_i} \\cdot (1 - P(x_i | C))^{(1 - f_i)} $\n",
    "\n",
    "Where $ f_i $ indicates whether the feature $ x_i $ is present (1) or absent (0).\n",
    "\n",
    "Example: In an email classifier, each word can either be present or absent. For each class (spam or not spam), compute the probabilities of each word appearing. When classifying a new email, apply the probabilities accordingly.\n",
    "Summary of Naive Bayes Types\n",
    "\n",
    "* Gaussian Naive Bayes: Used for continuous data and assumes a normal distribution of features.\n",
    "* Multinomial Naive Bayes: For discrete feature counts, especially suited for text classification.\n",
    "* Bernoulli Naive Bayes: Focuses on binary features (presence or absence).\n",
    "\n",
    "Each type has its own strengths and weaknesses based on the characteristics of the dataset in question. Understanding these distinctions ensures that the most suitable model is chosen for a given classification task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23640d4b-6ddc-47a6-a48d-39820233988e",
   "metadata": {},
   "source": [
    "4. Assumptions of Naive Bayes\n",
    "\n",
    "The Naive Bayes classifier is built upon several critical assumptions that simplify the computations involved in classification. Understanding these assumptions is essential because they have a significant impact on the model's performance under different conditions.\n",
    "4.1. Conditional Independence\n",
    "\n",
    "Overview: The most fundamental assumption of Naive Bayes is the conditional independence of features given the class label. This means that the presence (or absence) of a feature in the input data is assumed to be independent of another feature given the class label.\n",
    "\n",
    "Mathematical Representation: This can be mathematically expressed as:\n",
    "\n",
    "$ P(x_i | C, x_j) = P(x_i | C) \\quad \\forall i \\neq j $\n",
    "\n",
    "Where ( x_i ) and ( x_j ) are different features, and ( C ) is the class label. This key simplification allows us to calculate the joint likelihood of features as the product of their individual probabilities:\n",
    "\n",
    "$ P(X | C) = P(x_1 | C) \\cdot P(x_2 | C) \\cdots P(x_n | C) $\n",
    "\n",
    "Implications: While this assumption greatly reduces computational complexity, it may not hold true in many practical scenarios. For instance, in text classification, the words in a document often exhibit some level of dependency. However, in practice, Naive Bayes can still perform well even when this assumption is violated.\n",
    "\n",
    "4.2. Prior Probability Independence\n",
    "\n",
    "Overview: Naive Bayes also assumes that the prior probabilities of each class are independent of the feature values. Specifically, it assumes that the calculation of prior probabilities does not depend on the input features.\n",
    "\n",
    "Implication: This assumption makes it straightforward to calculate the prior probabilities ( P(C) ) from the training data without considering the feature values. However, this can lead to issues if the dataset is imbalanced, as one class may dominate the prior probabilities, thereby affecting the overall classification.\n",
    "4.3. Feature Relevance\n",
    "\n",
    "Overview: Naive Bayes implicitly assumes that all features contribute equally and independently to the decision-making process. It does not weigh one feature more heavily than another.\n",
    "\n",
    "Implication: In some cases, certain features might be more informative than others. Not accounting for the varying significance of features can lead to diminished performance in classification tasks where feature importance varies (for example, in medical diagnosis, certain symptoms carry more weight than others).\n",
    "\n",
    "4.4. No Correlation Among Features\n",
    "\n",
    "Overview: Following from the assumption of independence, Naive Bayes presumes that there are no correlations between the features. If features are correlated, the independence assumption could lead to inaccurate estimates of ( P(X | C) ).\n",
    "\n",
    "Implication: In real-world scenarios, especially in high-dimensional datasets, it’s common for features to be correlated. For example, in text classification, the presence of certain words may indicate the presence of others. Violations of this assumption can lead to reduced accuracy.\n",
    "Summary of Assumptions\n",
    "\n",
    "    Conditional Independence: Features are assumed to be independent given the class label. This is the crux of the \"naive\" aspect of Naive Bayes.\n",
    "    Prior Probability Independence: Class prior probabilities are assumed to be independent of the features.\n",
    "    Feature Relevance: All features contribute equally to the classification outcome.\n",
    "    No Correlation Among Features: Features are assumed not to exhibit correlation with each other.\n",
    "\n",
    "Practical Considerations for Assumptions\n",
    "\n",
    "While these assumptions simplify the model and make it computationally efficient, they can also lead to inaccuracies in scenarios where the assumptions don’t hold true. However, it is important to note that Naive Bayes often performs remarkably well even when these assumptions are violated. For many real-world applications, especially in text classification, Naive Bayes has proven to be a highly effective method despite its simplistic assumptions.\n",
    "\n",
    "Please let me know when you’re ready to move on to the Advantages and Disadvantages of the Naive Bayes classifier by typing NEXT!\n",
    "nurturegritSep 22, 2024\n",
    "\n",
    "NEXT\n",
    "gpt-4o-miniSep 22, 2024\n",
    "5. Advantages and Disadvantages of Naive Bayes\n",
    "\n",
    "Naive Bayes classifiers come with a set of strengths and weaknesses, making them more suitable for certain types of tasks while limiting their effectiveness in others. Let's explore the advantages and disadvantages in detail.\n",
    "5.1. Advantages of Naive Bayes Classifier\n",
    "\n",
    "* Simple and Fast:\n",
    "        The Naive Bayes algorithm is easy to understand and implement. The mathematical computations involved are not complex, allowing for quick training and classification, even on large datasets.\n",
    "        Since it operates on a probabilistic model, it can yield results in a matter of seconds, making it an efficient choice for real-time applications.\n",
    "\n",
    "* Effective with Large Datasets:\n",
    "        Naive Bayes works efficiently with large training datasets. Due to its simplicity, scaling it to large datasets is manageable without significant increases in computational cost.\n",
    "        The model’s performance generally improves with more data since it becomes more adept at estimating the probabilities required for classification.\n",
    "\n",
    "* Robust to Irrelevant Features:\n",
    "        The independence assumption makes Naive Bayes relatively robust to irrelevant features. Even if there are features that do not contribute to the outcome, they will not heavily impact the final prediction.\n",
    "        This quality can be particularly beneficial in applications like text classification, where many words may not be relevant to the classification task.\n",
    "\n",
    "* Works Well with High Dimensionality:\n",
    "        Naive Bayes is particularly suitable for problems with high-dimensional feature spaces (like text data), where the number of features can vastly outnumber the number of instances.\n",
    "        Its ability to handle high-dimensional spaces efficiently is one of the reasons it has remained popular in machine learning, especially for document and sentiment classification tasks.\n",
    "\n",
    "* Good Performance on Limited Data:\n",
    "        Despite its reliance on strong independence assumptions, Naive Bayes can perform quite well on smaller datasets, making it useful in scenarios where labeled data is scarce.\n",
    "\n",
    "* Probabilistic Output:\n",
    "        Naive Bayes provides a probability estimate for each class label, allowing the user to interpret the classifier's confidence in its predictions. This can be especially useful when making decisions that require an assessment of risk or likelihood.\n",
    "\n",
    "5.2. Disadvantages of Naive Bayes Classifier\n",
    "\n",
    "* Strong Independence Assumptions:\n",
    "        The most significant drawback of the Naive Bayes algorithm is its assumption of conditional independence among features. In practice, this assumption often does not hold, leading to suboptimal performance when features are correlated.\n",
    "        In cases where the independence assumption is significantly violated, the classifier's accuracy can dip markedly.\n",
    "\n",
    "* Limited Expressiveness:\n",
    "        Naive Bayes models the joint probability of features rather simply. Consequently, they may not capture complex relationships within the data that more sophisticated models can leverage effectively.\n",
    "        This limits the model’s expressiveness and its ability to model intricate feature interactions or non-linear relationships.\n",
    "\n",
    "* Zero Probability Problem:\n",
    "        The Naive Bayes algorithm can face issues when a certain feature value is not present in the training dataset for a specific class. This can lead to a zero probability estimation for that feature in that class, making it impossible to classify instances with those feature values.\n",
    "        This issue can be mitigated using techniques like Laplace (add-one) smoothing, which adjusts the probability estimates to avoid zeros.\n",
    "\n",
    "* Suboptimal for Small Datasets:\n",
    "        While Naive Bayes can work relatively well with fewer instances, if the dataset is too small, it may not provide accurate probability estimates for each feature and class combination. The algorithm's performance is highly contingent upon having sufficient data to form reliable probability distributions.\n",
    "\n",
    "* Sensitivity to Imbalanced Classes:\n",
    "        Naive Bayes can be sensitive to imbalanced class distributions, meaning if one class heavily outweighs another, the model may be biased towards the dominant class.\n",
    "        This issue can adversely impact performance in a real-world scenario where classes are often unbalanced, such as fraud detection or disease prediction.\n",
    "\n",
    "Summary of Advantages and Disadvantages\n",
    "\n",
    "Advantages:\n",
    "\n",
    "* Simple and efficient for large datasets.\n",
    "* Robust against irrelevant features.\n",
    "* Performs well in high-dimensional spaces.\n",
    "* Provides probabilistic outputs.\n",
    "\n",
    "Disadvantages:\n",
    "\n",
    "* Strong independence assumptions that may not hold.\n",
    "* Limited expressiveness, leading to oversimplified models.\n",
    "* Issues with zero probability for unseen feature/class combinations.\n",
    "* Sensitivity to imbalanced class distributions.\n",
    "\n",
    "Conclusion\n",
    "\n",
    "The Naive Bayes classifier can be an excellent choice for specific problems, especially when speed and simplicity are crucial. It shines in text-based applications and scenarios with significant dimensions. However, practitioners must also consider its limitations, particularly regarding feature independence and class imbalances."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e59dc1-f1a2-4d69-95a8-bd53c1258999",
   "metadata": {},
   "source": [
    "6. Applications of Naive Bayes\n",
    "\n",
    "Naive Bayes classifiers have found extensive use across a variety of fields and applications, largely due to their simplicity, speed, and effectiveness. Below, we explore several key areas where Naive Bayes excels.\n",
    "6.1. Text Classification\n",
    "\n",
    "Overview: One of the most common applications of Naive Bayes is in text classification tasks. This includes categorizing text documents into predefined categories.\n",
    "\n",
    "Examples:\n",
    "\n",
    "* Spam Detection:\n",
    "        Naive Bayes is extensively used in email filtering systems to classify emails as 'spam' or 'not spam.' By examining the frequency of certain words or phrases associated with spam, the classifier learns to predict whether new emails are likely to be unwanted messages.\n",
    "\n",
    "* Sentiment Analysis:\n",
    "        In sentiment analysis, Naive Bayes can be employed to determine the sentiment of a given piece of text (like reviews or comments) as positive, negative, or neutral. By analyzing word occurrences in labeled training datasets, the model can classify the sentiment of new text.\n",
    "\n",
    "* Topic Identification:\n",
    "        News articles or blog posts can be automatically classified into categories (such as politics, sports, technology) using Naive Bayes based on the frequency and distribution of words within each category.\n",
    "\n",
    "6.2. Document Classification\n",
    "\n",
    "Overview: Naive Bayes classifiers can be utilized to classify documents based on their content. Engaging in this task involves understanding and discerning between different textual features.\n",
    "\n",
    "Examples:\n",
    "\n",
    "    Document Categorization:\n",
    "        Organizations may need to categorize help desk tickets, support documents, or various reports into different categories (e.g., billing, technical support, customer issues) based on the content of the text.\n",
    "\n",
    "    Language Identification:\n",
    "        Naive Bayes can help in identifying the language of a document by learning the frequency of letters or words common in different languages. This can be useful for applications in translation services or multi-language support systems.\n",
    "\n",
    "6.3. Medical Diagnosis\n",
    "\n",
    "Overview: In the medical field, Naive Bayes can assist in diagnosing diseases based on observed symptoms. With its ability to compute probabilities, it can help medical practitioners differentiate between diseases based on patient data.\n",
    "\n",
    "Examples:\n",
    "\n",
    "    Disease Prediction:\n",
    "        By analyzing a dataset of patients with various symptoms and their diagnoses, Naive Bayes can learn the likely diseases associated with particular symptoms, helping doctors make informed decisions about potential conditions.\n",
    "\n",
    "    Patient Risk Assessment:\n",
    "        Healthcare practitioners can use Naive Bayes to predict patients' risk levels for certain diseases based on factors like age, lifestyle, and medical history.\n",
    "\n",
    "6.4. Recommendation Systems\n",
    "\n",
    "Overview: Naive Bayes can also play a role in creating recommendation systems by predicting user preferences based on previous behavior and features.\n",
    "\n",
    "Examples:\n",
    "\n",
    "    Content Recommendation:\n",
    "        E-commerce platforms can use Naive Bayes to suggest products to users based on their browsing history and purchase patterns. By analyzing previous purchases and associated features, the model can recommend items the user is likely to be interested in.\n",
    "\n",
    "    Movie Recommendations:\n",
    "        Streaming platforms can leverage Naive Bayes to suggest movies or TV shows based on the genres the user frequently watches. By classifying movie data with their attributes and the user's viewing history, the algorithm can recommend similar content.\n",
    "\n",
    "6.5. Intrusion Detection\n",
    "\n",
    "Overview: In cybersecurity, Naive Bayes classifiers can be utilized to identify suspicious activities in network traffic or system access.\n",
    "\n",
    "Examples:\n",
    "\n",
    "    Malware Detection:\n",
    "        By analyzing patterns in network traffic or file access, Naive Bayes can classify activities as benign or malicious. Training the model on known malware datasets and benign activities can help detect intrusions in real-time.\n",
    "\n",
    "    Fraud Detection:\n",
    "        Financial institutions utilize Naive Bayes for detecting potentially fraudulent transactions. By analyzing historical transaction data, the model can recognize patterns typical of fraud and flag suspicious transactions for further review.\n",
    "\n",
    "6.6. Market Basket Analysis\n",
    "\n",
    "Overview: Market basket analysis examines consumer purchasing patterns to understand correlations between products. Naive Bayes can help identify how purchasing one product may influence the purchase of another.\n",
    "\n",
    "Examples:\n",
    "\n",
    "    Product Bundling:\n",
    "        Retailers can utilize Naive Bayes to determine which items are commonly purchased together, aiding in product placement and promotional strategies.\n",
    "\n",
    "    Customer Segmentation:\n",
    "        Businesses can analyze data to segment customers based on buying behavior. By categorizing customers using Naive Bayes, retailers can tailor marketing campaigns to specific segments effectively.\n",
    "\n",
    "Summary of Applications\n",
    "\n",
    "    Text Classification: Email filtering, sentiment analysis, and document categorization.\n",
    "    Medical Diagnosis: Disease prediction and patient risk assessment.\n",
    "    Recommendation Systems: Suggesting products or content based on user behavior.\n",
    "    Intrusion Detection: Identifying suspicious network activities or fraudulent transactions.\n",
    "    Market Basket Analysis: Understanding purchasing patterns for better product placements.\n",
    "\n",
    "The versatility of Naive Bayes makes it an invaluable tool across various domains, particularly in situations where speed, simplicity, and scalability are paramount."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904eb45d-032b-46a3-9be2-6ef5f5565736",
   "metadata": {},
   "source": [
    "### 7. Examples and Implementation of Naive Bayes\n",
    "\n",
    "Now we'll dive into practical examples and implementation of the Naive Bayes classifier using commonly used libraries. We'll use Python and the popular libraries such as Scikit-learn to illustrate how Naive Bayes can be applied in real-world scenarios, particularly for text classification.\n",
    "7.1. Example: Spam Detection\n",
    "\n",
    "Overview: In this example, we’ll demonstrate how to implement a Naive Bayes classifier for detecting spam emails using the popular Natural Language Toolkit (NLTK) and Scikit-learn libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96eb081-34ec-441d-a18f-d31fa9d8d73e",
   "metadata": {},
   "source": [
    "Step 1: Data Preparation First, we need to gather a dataset of emails labeled as spam or not spam. A commonly used dataset is the Enron email dataset, but for simplicity, you can also create a small dataset manually. We'll start by creating a simple dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c77ad0-cb13-4578-9e72-05ea0b120ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = {\n",
    "    'text': [\n",
    "        'Congratulations! You have won a lottery.',\n",
    "        'Click here to claim your prize.',\n",
    "        'Dear user, your account has been updated.',\n",
    "        'Don’t miss our exclusive offer.',\n",
    "        'Important update regarding your account.',\n",
    "        'Meeting at 10 AM tomorrow.',\n",
    "        'Hi, how are you?',\n",
    "        'Cheap loan available now!'\n",
    "    ],\n",
    "    'label': ['spam', 'spam', 'ham', 'spam', 'ham', 'ham', 'spam']\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267d6554-5ae7-45e4-b03c-32a9a2eff68b",
   "metadata": {},
   "source": [
    "Step 2: Data Preprocessing Convert the text data into a format suitable for Naive Bayes. We'll utilize the CountVectorizer from Scikit-learn to transform the text into a bag-of-words representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3926831-1604-4c63-9691-9d5e913c2cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Text Vectorization\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(df['text'])\n",
    "\n",
    "# Labels\n",
    "y = df['label']\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a1444b-200a-4f1d-ba8f-242312aa2bef",
   "metadata": {},
   "source": [
    "Step 3: Implementing the Naive Bayes Classifier We’ll use the MultinomialNB classifier available in Scikit-learn, which is appropriate for text classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60cf331f-0399-4641-b8f3-cbfb3b2caaa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Create the Naive Bayes classifier\n",
    "classifier = MultinomialNB()\n",
    "\n",
    "# Train the classifier\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622bb3ea-127e-4402-bd23-1e63e8db0755",
   "metadata": {},
   "source": [
    "Step 4: Making Predictions After training, we can then evaluate the classifier's performance on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d0fea0-b30f-456b-9be0-5c365d88ce88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting on the test set\n",
    "predictions = classifier.predict(X_test)\n",
    "\n",
    "# Checking predictions\n",
    "for text, prediction in zip(X_test, predictions):\n",
    "    print(f'Text: \"{vectorizer.inverse_transform(text)[0]}\"\\nPredicted: {prediction}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7656ed-e7a2-43fb-977b-56f37eee03bb",
   "metadata": {},
   "source": [
    "Step 5: Evaluating Performance We can measure the performance of our naive model using metrics such as accuracy, precision, recall, and F1-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f98ca6-ebc5-42fe-bce1-b080b6dad352",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Display classification report\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d18c047-6119-4a9b-b209-43b6659c419e",
   "metadata": {},
   "source": [
    "7.2. Example: Sentiment Analysis\n",
    "\n",
    "In this example, we’ll perform sentiment analysis on a dataset of movie reviews. The dataset consists of labeled positive and negative reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59cd0d5-15d4-45bc-9b85-157e35f546ec",
   "metadata": {},
   "source": [
    "Step 1: Load the Dataset You can use the movie_reviews dataset from the NLTK library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409e886d-ff87-43d6-9310-bd04fbb2ff4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import movie_reviews\n",
    "import random\n",
    "\n",
    "nltk.download('movie_reviews')\n",
    "\n",
    "# Load movie reviews\n",
    "documents = [(list(movie_reviews.words(fileid)), category)\n",
    "             for category in movie_reviews.categories()\n",
    "             for fileid in movie_reviews.fileids(category)]\n",
    "\n",
    "random.shuffle(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e110a0a1-2d44-4d4a-b483-158ba769856c",
   "metadata": {},
   "source": [
    "Step 2: Data Preprocessing Convert the words into a bag-of-words format and create features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c34b897-3b42-444d-8684-b2f1ad60521e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all words in the movie reviews\n",
    "all_words = nltk.FreqDist(w.lower() for w in movie_reviews.words())\n",
    "word_features = list(all_words)[:2000]\n",
    "\n",
    "def document_features(document):\n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features[f'contains({word})'] = (word in document_words)\n",
    "    return features\n",
    "\n",
    "featuresets = [(document_features(doc), category) for (doc, category) in documents]\n",
    "train_set, test_set = train_test_split(featuresets, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a08e1d1c-bb7c-40af-808f-419df3b60a1a",
   "metadata": {},
   "source": [
    "Step 3: Implementing the Naive Bayes Classifier Now, we’ll create and train the Naive Bayes classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05a2dd1-c951-4501-9bd4-8da15157324b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.classify import NaiveBayesClassifier\n",
    "\n",
    "# Create the classifier\n",
    "classifier = NaiveBayesClassifier.train(train_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abce22be-548d-4adf-a8f9-0d77fc1206ed",
   "metadata": {},
   "source": [
    "Step 4: Making Predictions We can evaluate the classifier’s performance on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7209d8ef-d36f-4776-97cb-7af1897f8fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy\n",
    "accuracy = nltk.classify.accuracy(classifier, test_set)\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
    "\n",
    "# Display the top features\n",
    "classifier.show_most_informative_features(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14ff34c-5b3b-49ea-bfcd-82dccbc0a3a6",
   "metadata": {},
   "source": [
    "Summary of Examples\n",
    "\n",
    "    Spam Detection: Implemented using the Multinomial Naive Bayes classifier with a feature set derived from text documents.\n",
    "    Sentiment Analysis: Utilized the movie reviews dataset and trained a Naive Bayes classifier to predict the sentiment of reviews based on word presence.\n",
    "\n",
    "Conclusion\n",
    "\n",
    "The Naive Bayes classifier is a powerful tool particularly suited for classification tasks involving text data. Its implementation in Python is straightforward, thanks to libraries like Scikit-learn and NLTK, which simplify both data preprocessing and modeling. Understanding Naive Bayes can enhance your capability in handling a wide array of classification problems effectively and efficiently.\n",
    "\n",
    "This concludes the overview of Naive Bayes Classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0641fe2a-d37c-4e1c-bddb-3e97f4de6b6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65241881-f860-4f2b-8d3a-e7b6a45f36db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
