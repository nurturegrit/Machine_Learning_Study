{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9dcac9fa-5855-42e0-9f33-b02147052c07",
   "metadata": {},
   "source": [
    "## Worst Case (Online) Learning\n",
    "\n",
    "### Subtopics:\n",
    "1. Introduction to Online Learning\n",
    "2. Worst Case Analysis in Learning\n",
    "3. Online Algorithms\n",
    "4. Regret Minimization\n",
    "5. Applications of Online Learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2139d9-4398-49ba-a400-1bcb35979eba",
   "metadata": {},
   "source": [
    "# 1. Introduction to Online Learning\n",
    "\n",
    "Online learning is a paradigm of machine learning where the model is trained incrementally on data that arrives sequentially, as opposed to the traditional batch learning where the entire dataset is available at once. This approach has several advantages, especially in scenarios where data is continuously generated, and it can be impractical or impossible to store and process all previous data.\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "#### 1.1. Incremental Learning\n",
    "In online learning, the model updates itself using one observation (or a small batch) at a time. This allows for:\n",
    "- **Real-time processing**: The model adapts immediately to new data, making it useful for applications like stock prices or web traffic prediction.\n",
    "- **Memory Efficiency**: Since online models don't require all past data, they can operate within the limits of available memory.\n",
    "\n",
    "#### 1.2. Temporal Dependency\n",
    "Online learning often takes advantage of the temporal nature of data. For instance:\n",
    "- In stock market predictions, recent prices could be more significant than older ones.\n",
    "- In user recommendation systems, user preferences can change over time, making adjustments in real-time essential.\n",
    "\n",
    "#### 1.3. Concept Drift\n",
    "Concept drift refers to changes in the underlying data distribution over time. This poses challenges for online learning models as their performance may degrade if they don’t adapt to these changes. The key considerations include:\n",
    "- **Detection**: Identifying when concept drift has occurred.\n",
    "- **Adaptation**: Updating the learning model efficiently to account for new trends.\n",
    "\n",
    "#### 1.4. Performance Measurement\n",
    "Performance in online learning is often assessed using cumulative loss over time. For a given instance \\( x_t \\) with a true label \\( y_t \\) returning a prediction \\( \\hat{y}_t \\), the loss at time \\( t \\) can be defined as:\n",
    "\n",
    "$$ \\text{Loss}(t) = L(y_t, \\hat{y}_t) $$\n",
    "\n",
    "where \\( L \\) could be various loss functions like mean squared error (MSE) for regression or binary cross-entropy for classification.\n",
    "\n",
    "### Online Learning Models\n",
    "\n",
    "1. **Stochastic Gradient Descent (SGD)**: A popular method in online learning where the model parameters are updated using only the current data point. The update rule can be defined as:\n",
    "\n",
    "   $$ \\theta_{t+1} = \\theta_t - \\eta \\nabla L(\\theta_t; x_t, y_t) $$\n",
    "\n",
    "   where \\( \\eta \\) is the learning rate, \\( x_t \\) is the input feature, and \\( y_t \\) is the target label.\n",
    "\n",
    "2. **Adversarial Learning**: An approach where the online learner interacts with an adversary that produces sequences of input data to minimize the learner's performance. This concept leads us to explore the worst case in online learning.\n",
    "\n",
    "### Worst Case Analysis\n",
    "\n",
    "In the context of online learning, the \"worst case\" refers to scenarios where the model must perform optimally despite potentially adversarial input sequences. This often involves:\n",
    "\n",
    "- **Robustness**: Ensuring that the model performs reasonably even under the least favorable conditions.\n",
    "- **Bounded Performance**: The concept connects closely with regret minimization, where the goal is to keep track of how poorly the online learner performs relative to a fixed strategy and strives to minimize this regret.\n",
    "\n",
    "### Summary\n",
    "\n",
    "Online learning allows for incremental updates to models based on sequential data. The worst case analysis is a crucial aspect of this domain, ensuring models can handle challenging scenarios to maintain performance over time.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc2e80e-8352-49c1-bfa4-40fb0477cd06",
   "metadata": {},
   "source": [
    "# 2. Worst Case Analysis in Learning\n",
    "\n",
    "In the realm of machine learning, particularly in online learning, worst case analysis is an essential strategy that helps us understand how well a learning algorithm can perform under unfavorable conditions. This analysis is crucial for developing robust algorithms that maintain performance even when faced with highly challenging or adversarial data inputs.\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "#### 2.1. Adversarial Model\n",
    "The adversarial model is a framework where the performance of an algorithm is assessed against the most challenging scenarios posed by an 'adversary'. The adversary or opponent is assumed to have complete knowledge of the online learner's algorithm and can produce data inputs optimized to maximize the learner's error.\n",
    "\n",
    "##### Characteristics:\n",
    "- **Dynamic Challenges**: The adversary has the flexibility to choose their data inputs dynamically, which forces the online learner to continuously adapt.\n",
    "- **No Assumptions on Data Distribution**: In the worst-case scenario, the algorithm cannot rely on any distributional assumptions about the incoming data, making it necessary to be prepared for a wide range of potential inputs.\n",
    "\n",
    "#### 2.2. Performance Metrics in Adversarial Settings\n",
    "The primary focus of worst-case analysis is typically on the idea of regret, which measures how much worse the online learning algorithm performs compared to the best fixed strategy in hindsight. \n",
    "\n",
    "The **regret** at time \\( T \\) can be defined as:\n",
    "\n",
    "$$ R(T) = \\sum_{t=1}^{T} L(y_t, \\hat{y}_t) - \\min_{a \\in A} \\sum_{t=1}^{T} L(y_t, a) $$\n",
    "\n",
    "where:\n",
    "\n",
    "- \\( L(y_t, \\hat{y}_t) \\) is the loss incurred by the online learner at time \\( t \\),\n",
    "- The term \\( \\min_{a \\in A} \\sum_{t=1}^{T} L(y_t, a) \\) represents the cumulative loss incurred by the best fixed-algorithm \\( a \\) over the same time horizon.\n",
    "\n",
    "#### 2.3. Bounded Regret\n",
    "In a worst-case scenario, the goal is to ensure that the algorithm's regret is bounded, meaning it does not grow too fast and remains manageable even as \\( T \\) increases. This leads to strategies for minimizing regret, where an online algorithm will aim to achieve performance in the sample complexity close to that of the optimal offline learner.\n",
    "\n",
    "A common bound that is derived is:\n",
    "\n",
    "$$ R(T) \\leq C \\cdot \\log(T) $$\n",
    "\n",
    "for some constant \\( C \\). Achieving logarithmic regret is often seen as a benchmark of efficiency, indicating that although an online algorithm is learning at each time step, its performance closely mirrors that of a static expert.\n",
    "\n",
    "#### 2.4. Strategies for Minimizing Worst-Case Regret\n",
    "Several strategies have been proposed to minimize worst-case regret in online learning:\n",
    "\n",
    "1. **Follow the Leader**: This strategy identifies the best-performing action from the past data and continues to choose that action unless a better alternative arises. However, it can lead to poor performance in dynamic environments if the data distribution changes significantly.\n",
    "\n",
    "2. **Exponential Weights**: Here, the algorithm assigns weights to different actions based on their performance, recalling that actions with lower loss should be weighted more heavily in subsequent decisions. The update rule for the weight \\( w_t \\) for action \\( a \\) can be expressed as:\n",
    "   \n",
    "   $$ w_{t+1}(a) = w_t(a) \\cdot e^{-\\eta L(y_t, a)} $$\n",
    "\n",
    "   This technique often helps achieve sublinear regret, as it effectively balances taking risks against exploiting known information.\n",
    "\n",
    "3. **Stochastic Algorithms**: Often, randomness can be introduced in decision-making to escape local optima and explore a broader action space. This strategy can mitigate the risk of being exploited by adversarial inputs.\n",
    "\n",
    "### Summary\n",
    "\n",
    "Worst-case analysis in learning provides a framework for evaluating the robustness of online algorithms against adversarial conditions. By focusing on performance metrics like regret, we can derive principles and strategies to equip our learning models against the most challenging inputs, thereby ensuring stable performance across diverse scenarios.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7661cccf-25b5-4680-b2bd-964801d574a1",
   "metadata": {},
   "source": [
    "# 3. Online Algorithms\n",
    "\n",
    "Online algorithms are designed to process data sequentially, making decisions based on the information available at the moment without knowledge of future data. This characteristic sets online algorithms apart from traditional batch algorithms that learn from a complete dataset.\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "#### 3.1. Definition and Characteristics\n",
    "An online algorithm develops a solution iteratively as each data point arrives. Key characteristics include:\n",
    "\n",
    "- **Sequential Processing**: The algorithm must make decisions with partial information, akin to real-world applications like financial markets, where data comes in real-time.\n",
    "- **Immediate Output**: Once new data is available, the algorithm yields predictions or decisions instantly, rather than waiting to analyze a full dataset.\n",
    "- **Adaptability**: Online algorithms must be flexible to adapt to changes in data distributions, particularly when faced with concept drift.\n",
    "\n",
    "#### 3.2. Types of Online Algorithms\n",
    "Online algorithms can be categorized into several types, notably, depending on their strategy for tackling incoming data:\n",
    "\n",
    "1. **Greedy Algorithms**: These algorithms make the locally optimal choice at each step with the hope of finding a global optimum. For example, in a scheduling problem, a greedy algorithm would always select the next task with the earliest deadline.\n",
    "\n",
    "2. **Competitive Algorithms**: These algorithms assess their performance against a fixed benchmark or 'competitor'. The competitive ratio measures the worst-case performance of the online algorithm in relation to this benchmark. If \\( A \\) is the online algorithm and \\( B \\) is a static benchmark, the competitive ratio \\( C \\) is defined as:\n",
    "\n",
    "   $$ C = \\sup_{x} \\frac{\\text{Cost}(A, x)}{\\text{Cost}(B, x)} $$\n",
    "\n",
    "   A competitive algorithm achieves a good ratio if it maintains low costs in relation to the static benchmark over varying scenarios.\n",
    "\n",
    "3. **Randomized Algorithms**: These algorithms incorporate randomization into decision-making for better performance in adversarial scenarios. They make decisions based on probabilities rather than strict rule-based performance, providing flexibility to explore unknown action spaces.\n",
    "\n",
    "#### 3.3. Regret and Performance Evaluation\n",
    "As online algorithms operate without knowledge of future examples, assessing their performance typically hinges upon the concept of regret. The **cumulative regret** serves as a way to evaluate the decision-making quality of the algorithm.\n",
    "\n",
    "For a learning algorithm with \\( T \\) sequentially observed data points, the total regret can be formalized as:\n",
    "\n",
    "$$ R(T) = \\sum_{t=1}^{T} L(y_t, \\hat{y}_t) - \\min_{a \\in A} \\sum_{t=1}^{T} L(y_t, a) $$\n",
    "\n",
    "This measure examines how much worse the online algorithm performs compared to the best possible action in hindsight (i.e., the optimal static strategy).\n",
    "\n",
    "#### 3.4. Applications of Online Algorithms\n",
    "Online algorithms find numerous applications across different domains, proving their utility in various real-world scenarios:\n",
    "\n",
    "1. **Financial Trading**: Online algorithms can adapt to fluctuating markets and make buy/sell decisions in real-time based on incoming prices and trends.\n",
    "\n",
    "2. **Website Recommendation Systems**: As users interact with websites, online algorithms modify recommendations instantly to align with user preferences, improving the user experience.\n",
    "\n",
    "3. **Adaptive Control Systems**: In engineering, online algorithms adjust control parameters based on real-time feedback, optimizing performance.\n",
    "\n",
    "4. **Network Routing**: Online algorithms determine packet routing in networks where the flow of data is unpredictable, ensuring efficient transmission of information.\n",
    "\n",
    "### Summary\n",
    "Online algorithms are essential for processing sequential data while making immediate decisions. Their adaptability, sequential processing, and reliance on regret metrics ensure efficient performance even in unpredictable environments. By understanding their types, strengths, and applications, we can leverage online algorithms to tackle a range of real-world problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "974f127c-0c10-4c4f-8be9-4fb28e7458f7",
   "metadata": {},
   "source": [
    "# 4. Regret Minimization\n",
    "\n",
    "Regret minimization is a central concept in online learning and online algorithms. It serves as a metric to evaluate and improve the performance of an online algorithm against some benchmark, typically a static or optimal strategy. The objective is to ensure that the cumulative loss of an online learner is kept as low as possible compared to the best fixed strategy it could have followed in hindsight.\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "#### 4.1. Definition of Regret\n",
    "Cumulative regret, denoted as \\( R(T) \\), can be expressed mathematically as:\n",
    "\n",
    "$$ R(T) = \\sum_{t=1}^{T} L(y_t, \\hat{y}_t) - \\min_{a \\in A} \\sum_{t=1}^{T} L(y_t, a) $$\n",
    "\n",
    "Where:\n",
    "- \\( L(y_t, \\hat{y}_t) \\) is the loss incurred by the online learner at time \\( t \\).\n",
    "- The term \\( \\min_{a \\in A} \\sum_{t=1}^{T} L(y_t, a) \\) represents the cumulative loss of the best fixed decision from the set of possible actions \\( A \\).\n",
    "\n",
    "**Interpretation**: The regret quantifies the difference in performance between the online learner and the best static policy. A lower regret means that the online learner performs comparably to an optimal static strategy.\n",
    "\n",
    "### Measurement of Regret\n",
    "\n",
    "#### 4.2. Types of Regret\n",
    "1. **Static Regret**: This is calculated over a specific sequence of observations and does not adapt as the model continues learning.\n",
    "  \n",
    "2. **Dynamic Regret**: This measurement modifies the benchmark over time. It allows for adjustments based on the changing landscape of the problem, potentially yielding lower regret in environments with significant changes.\n",
    "\n",
    "3. **Final vs. Cumulative Regret**: \n",
    "   - **Final Regret** considers only the last time step.\n",
    "   - **Cumulative Regret** aggregates regret over the entire prediction horizon and is more commonly used in the literature.\n",
    "\n",
    "#### 4.3. Regret Bounds\n",
    "Regret bounds are essential to understand the efficiency of an online learning algorithm. Achieving a logarithmic regret bound, \\( R(T) \\leq O(\\log T) \\), is often desired. \n",
    "\n",
    "- **Theoretical Implications**: In practical terms, a logarithmic bound suggests that as the number of instances increases, the average regret per instance grows slower than the number of instances, indicating efficient learning.\n",
    "  \n",
    "- **Empirical Implications**: A lower bound on regret ensures that the online learner can keep up with the performance of the best static learner under various requirements.\n",
    "\n",
    "### Techniques for Regret Minimization\n",
    "\n",
    "#### 4.4. Algorithmic Approaches\n",
    "1. **Follow the Regularized Leader (FRL)**:\n",
    "   - In this approach, the online algorithm selects an action based on past performance but regularized by a penalty term to discourage overfitting.\n",
    "   - The update rule can be defined as:\n",
    "\n",
    "   $$ \\theta_{t+1} = \\arg\\min_{\\theta} \\left( \\sum_{s=1}^{t} L(y_s, \\hat{y}_s; \\theta) + \\lambda R(\\theta) \\right) $$\n",
    "\n",
    "   where \\( R(\\theta) \\) is a regularization term, and \\( \\lambda \\) controls the trade-off between learning and regularization.\n",
    "\n",
    "2. **Meta-Algorithms**:\n",
    "   - These algorithms, such as Online Gradient Descent and Hedge, use past performance to weigh future decisions. For instance:\n",
    "     \n",
    "   $$ w_{t+1} = w_t \\cdot e^{-\\eta L(y_t, \\hat{y}_t)} $$\n",
    "\n",
    "   This elegantly balances the tension between exploration and exploitation.\n",
    "\n",
    "3. **Stochastic Mirror Descent**:\n",
    "   - This advanced approach combines concepts from optimization and probability. It minimizes a dual function iteratively and is particularly effective in high-dimensional spaces.\n",
    "\n",
    "#### 4.5. Practical Considerations\n",
    "1. **Learning Rate**: The choice of learning rate \\( \\eta \\) can significantly influence the performance of the learning algorithm and its ability to minimize regret.\n",
    "\n",
    "2. **Feature Representation**: Effective feature engineering and representation can directly impact an algorithm's performance and, consequently, its regret.\n",
    "\n",
    "3. **Evaluation Techniques**: Regular cross-validation and testing on real-world datasets can help refine models to minimize regret in practical applications.\n",
    "\n",
    "### Applications of Regret Minimization\n",
    "1. **Adaptive Learning Systems**: In personalized recommendations, the ability to minimize regret means that the system can adapt more responsively to user preferences over time.\n",
    "\n",
    "2. **Resource Allocation**: In network routing and resource management, minimizing regret can help optimize networks against unfavorable conditions, thereby maintaining efficiency.\n",
    "\n",
    "3. **Game Theory**: Regret minimization is extensively used in strategic decision-making, where players aim to reduce their losses against opponents’ strategies over time.\n",
    "\n",
    "### Summary\n",
    "Regret minimization is a fundamental concept in online learning, providing a mechanism to evaluate and improve performance relative to fixed benchmarks. Various techniques and algorithms are available to achieve low regret, making it possible for online learners to adapt efficiently and maintain robust performance even under challenging environments.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa473e13-b945-47b0-9b45-9a2b1ace0071",
   "metadata": {},
   "source": [
    "# 5. Applications of Online Learning\n",
    "\n",
    "Online learning has diverse applications across various fields, harnessing the strengths of sequential data processing and adaptability. By examining some key use cases, we can appreciate how online learning can optimize performance, make predictions, and improve decision-making in real-time scenarios.\n",
    "\n",
    "### Key Applications of Online Learning\n",
    "\n",
    "#### 5.1. Financial Markets\n",
    "In financial trading, the stock market is a classic example where data is continuously generated. Online learning algorithms can optimize trading strategies by dynamically adjusting to new market information while minimizing risks and maximizing returns.\n",
    "\n",
    "##### Example:\n",
    "- **Algorithmic Trading**: Algorithms estimate stock movements based on incoming data such as price changes, volume, and news sentiments. Techniques like stochastic gradient descent help model patterns, allowing traders to buy/sell stocks efficiently.\n",
    "\n",
    "**Challenges**:\n",
    "- **High Volatility**: Financial markets can change rapidly; hence, models must adapt quickly to minimize regret while ensuring stability.\n",
    "- **Noise in Data**: Distinguishing meaningful signals from noise is crucial for effective predictions.\n",
    "\n",
    "#### 5.2. Recommendation Systems\n",
    "E-commerce platforms utilize online learning to deliver personalized recommendations based on user behavior. As users interact with products, online algorithms adjust recommendations in real-time to enhance user experiences and engagement.\n",
    "\n",
    "##### Example:\n",
    "- **Content-Based Filtering**: Algorithms analyze user preferences and suggest items that have similar characteristics to those previously liked by the user. If a user frequently selects action movies, the system learns to propose additional titles in that genre.\n",
    "\n",
    "- **Collaborative Filtering**: Leverages user interactions with items to recommend items liked by similar users. \n",
    "\n",
    "**Challenges**:\n",
    "- **Cold Start Problem**: New users or items lack historical data, making initial recommendations less effective.\n",
    "- **Changing User Preferences**: Users’ tastes evolve over time, necessitating models that can adapt to these changes efficiently.\n",
    "\n",
    "#### 5.3. Online Advertising\n",
    "In online advertising, determining which ads to display to users based on their behavior and preferences can be optimized using online learning algorithms. \n",
    "\n",
    "##### Example:\n",
    "- **Real-Time Bidding**: Ads are auctioned in real-time to show targeted advertisements to users based on their interests. Online algorithms evaluate user interactions with ads to adjust bidding strategies over time.\n",
    "\n",
    "**Challenges**:\n",
    "- **Privacy Concerns**: Collecting user data raises ethical and legal issues. Ad platforms must navigate these while providing effective targeting.\n",
    "- **Ad Fatigue**: Users may become less responsive to repeated ad exposure; algorithms should account for this to maintain engagement.\n",
    "\n",
    "#### 5.4. Robotics and Autonomous Systems\n",
    "Robots that interact with dynamic environments can benefit greatly from online learning. These systems can learn from real-time feedback, allowing them to navigate and adapt to obstacles on-the-fly.\n",
    "\n",
    "##### Example:\n",
    "- **Autonomous Vehicles**: Vehicles can learn about road situations based on sensor input and adjust their driving behavior. Online algorithms help in making strictly immediate decisions based on sensor data (like avoiding obstacles).\n",
    "\n",
    "**Challenges**:\n",
    "- **Safety and Reliability**: The algorithm must make correct decisions under all circumstances, as incorrect actions can lead to accidents.\n",
    "- **Complex Environmental Dynamics**: Algorithms must handle rich and complex data streams from sensors, which can vary substantially over time.\n",
    "\n",
    "#### 5.5. Natural Language Processing (NLP)\n",
    "Online learning is pivotal in NLP, where language evolves, and models require ongoing updates based on new text data.\n",
    "\n",
    "##### Example:\n",
    "- **Sentiment Analysis**: Models learn to gauge user sentiments based on incoming social media posts or reviews. As new expressions arise, the algorithm adapts to maintain accuracy in predictions.\n",
    "\n",
    "**Challenges**:\n",
    "- **Ambiguity and Context**: The meaning of words can change based on context, increasing the complexity of modeling language dynamically.\n",
    "- **Scalability**: Handling the continuous influx of data while ensuring rapid updates can be resource-intensive.\n",
    "\n",
    "### Summary\n",
    "Online learning applications span multiple domains, including finance, e-commerce, robotics, and natural language processing. These applications leverage the capabilities of online algorithms to adapt and optimize performance in real-time, addressing various challenges while continuously enhancing user experience and operational efficiency.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3727faf0-6ca5-4542-bb47-3ddc5c1498a7",
   "metadata": {},
   "source": [
    "# Summary of Worst Case (Online) Learning\n",
    "\n",
    "Having explored the various subtopics under \"Worst Case (Online) Learning,\" let's summarize the key points, ensuring a comprehensive understanding of the entire topic.\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Online Learning Fundamentals**:\n",
    "    - Online learning is an incremental approach to training models where data arrives sequentially.\n",
    "    - Unlike batch learning, online learning allows for immediate adjustments to models based on new information.\n",
    "\n",
    "2. **Worst Case Analysis**:\n",
    "    - This analysis assesses how well online algorithms perform in the presence of adversarial inputs.\n",
    "    - Regret minimization is a critical concept, quantifying the performance of an online learner compared to the best possible strategy in hindsight.\n",
    "\n",
    "3. **Online Algorithms**:\n",
    "    - Online algorithms differ from traditional methods in that they operate without full knowledge of incoming data.\n",
    "    - They are characterized by their sequential decision-making and adaptability.\n",
    "\n",
    "4. **Regret Minimization Techniques**:\n",
    "    - Various algorithms and methods allow for regret minimization, such as Follow the Regularized Leader, Exponential Weights, and Stochastic Mirror Descent.\n",
    "    - Achieving low cumulative regret is crucial for maintaining competitiveness against static benchmarks.\n",
    "\n",
    "5. **Real-World Applications**:\n",
    "    - Online learning algorithms are widely applied in sectors like finance (algorithmic trading), recommendations (e-commerce), advertising, robotics, and natural language processing.\n",
    "    - These applications benefit from real-time data processing, enabling models to respond dynamically to changing conditions.\n",
    "\n",
    "### Conclusion\n",
    "Understanding worst-case online learning equips us with the theoretical framework and practical skills necessary to address complex, real-time scenarios using machine learning. From financial trading strategies to personalized recommendations, the principles of online learning play a vital role in enhancing decision-making processes.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6afd45-019d-4334-9bf4-54bce97db636",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
